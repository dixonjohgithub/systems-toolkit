# MVP0 — Intake Quality: System-Level Framing

## The One-Sentence Version

**Intake quality is not a form problem — it is a portfolio governance problem. Every low-quality submission that enters the review chain consumes organizational capacity not on evaluation, but on information retrieval.**

---

## Why This Is a System-Level Problem

Most organizations frame intake quality as a submitter behavior problem. The submitter didn't fill out the form properly. The submitter was vague. The submitter didn't follow the template. The solution, in this framing, is better forms, clearer instructions, more training.

This framing is wrong — and it explains why the problem never gets solved.

The real dynamic is structural. When a low-quality submission enters the review chain, the cost doesn't fall on the submitter. It falls on everyone above them. The reviewer who receives an incomplete intake now has three options: reject it outright, approve it with incomplete information, or begin a clarification cycle. Most reviewers choose the clarification cycle — it feels like the responsible option, and it is. But that cycle has a cost that is never measured, never attributed to the original submission, and never visible in aggregate to anyone with the authority to fix it.

This is the system-level failure: **the cost of poor quality is paid by the people with the least visibility into it, and invisible to the people with the authority to address it.**

---

## The Visibility Asymmetry

At the intake reviewer level, a clarification cycle is Tuesday. It is part of the job. There is no mechanism that flags a submission as "this took four back-and-forth rounds before it was actionable." There is no penalty for the submitter. There is no cost attributed to the intake itself. The reviewer absorbs the inefficiency and moves on.

At the portfolio level, something different is happening — but it's not visible as an intake quality problem. Leadership sees:

- Review queues that move slower than expected
- Senior reviewers consistently over-capacity
- Time-to-approval metrics that defy the theoretical throughput of the review process
- Strategic initiatives stalled in intake, not because of resource constraints on the initiative itself, but because intake review is consuming capacity that should be applied to evaluation

None of this surfaces as "we have an intake quality problem." It surfaces as "our review process is slow" or "our teams are overwhelmed." The root cause — that a significant fraction of review capacity is being spent retrieving information rather than evaluating submissions — is invisible because no one is measuring it.

This is what makes it a portfolio governance problem. **The cost is real and it is aggregate, but it is absorbed locally and silently at every level of the review chain before it ever reaches the people who could address it systemically.**

---

## What "Review Capacity" Actually Means at Scale

When a reviewer receives an incomplete submission and begins a clarification cycle, they are not just spending time on that submission. They are:

- Holding that submission in working memory across days or weeks while they wait for responses
- Re-reading context they already processed to re-establish it on each re-engagement
- Coordinating scheduling across multiple parties for clarification calls or reviews
- Making decisions downstream with less information than they should have — because at some point, the cycle ends not with a complete submission but with a "good enough" one

Multiply this across an organization with 120+ active intake processes and a portfolio of hundreds of in-flight submissions. The aggregate effect is not a minor inefficiency. It is a systematic misallocation of the organization's most expensive resource: the attention and judgment of the people qualified to evaluate these submissions.

Senior reviewers, committee members, and executives who sit on intake review panels are not cheap capacity. When that capacity is spent on information retrieval rather than evaluation, the cost is real — it simply doesn't appear on any report, because no one has connected the intake quality event to the downstream review capacity event.

---

## Why Individual Reviewers Don't Feel the Pain Acutely

There is no penalty mechanism in the current system. A reviewer who processes an incomplete submission and runs three clarification cycles is not worse off than one who receives a complete submission. They are both doing their jobs. The clarification cycle is experienced as part of normal operations, not as an anomaly to be flagged or measured.

This is actually the correct response given the system they're operating in. The reviewer is not the problem. They are absorbing a cost created elsewhere in the system, and the system gives them no tool to surface that cost upstream.

The absence of penalty is not neutral. It means the system has no feedback mechanism. There is nothing that signals to the submitter that their incomplete intake created cost downstream. There is nothing that signals to the intake designer that their form is generating low-quality submissions. There is nothing that signals to leadership that review capacity is being systematically consumed by information retrieval.

**A system with no feedback on quality will produce inconsistent quality. Not because the people in it are careless, but because the system gives them no signal that quality matters.**

---

## The Portfolio Governance Frame

From a portfolio governance perspective, intake quality is a resource allocation problem with a specific structure:

**The resource is review capacity.** Senior reviewers, intake committees, executive sponsors — these are finite, expensive, and their attention is the limiting factor in how fast the portfolio can move.

**The allocation is wrong.** A meaningful fraction of that capacity is being spent not on evaluation — the activity it is meant to perform — but on information retrieval: asking for what should have been provided, waiting for responses, re-establishing context across gaps in the clarification cycle.

**The misallocation is invisible.** Because intake quality events and review capacity consumption events are never connected in any reporting system, leadership sees the effect (slow queues, overwhelmed reviewers, stalled initiatives) but not the cause.

**The fix is upstream.** The only lever that addresses this at the system level is ensuring that submissions enter the review chain complete and actionable. Not as a nice-to-have. As a hard prerequisite. Every submission that clears the quality gate before reaching a reviewer is a submission that uses review capacity for its intended purpose: evaluation and decision.

This is what MVP0 does. Not quality for its own sake. Quality as a prerequisite for portfolio efficiency — so that the organization's most expensive review capacity is applied to the decisions it was designed to make, not to the information retrieval that should have been completed before the submission arrived.

---

## The Single Framing Statement for Executive Audiences

> *"Right now, a portion of our intake review capacity — the attention of the people we most need focused on portfolio decisions — is being consumed by clarification cycles on incomplete submissions. That cost is paid silently, locally, and without attribution at every level of the review chain. It never appears on a report. It never triggers a flag. It compounds across every submission that enters the system below the quality bar. MVP0 is the governance layer that moves that quality bar upstream — so review capacity is spent on evaluation, not on information retrieval."*

---

## One-Line Versions by Audience

| Audience | Framing |
|----------|---------|
| Executive / Portfolio Owner | "Right now, your most expensive review capacity is being spent on information retrieval, not evaluation. MVP0 fixes the upstream source." |
| VP / Senior Director | "Reviewers are absorbing a cost that has no feedback mechanism — they do the clarification cycle, the submitter never knows, and nothing changes. MVP0 creates the signal." |
| Program / Portfolio Manager | "You cannot measure throughput accurately when a fraction of every review engagement is untracked information retrieval. MVP0 makes every submission accountable to a quality standard before it enters the queue." |
| Intake Reviewer | "The back-and-forth you do today to get a submission actionable goes away. Every intake you receive will have already been validated — complete, specific, and ready to evaluate." |

---

*This framing is intended to position MVP0 not as a form improvement, but as a portfolio governance mechanism. The quality standard is not about better UX for submitters — it is about ensuring that the organization's intake review investment is applied to evaluation, not to information retrieval.*
